{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "import glob\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "import preprocess_data as prep\n",
    "from caffe_classes import class_names\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Path to bladder cancer training data\n",
    "data_path = '/Users/vladarozova/Dropbox/Bladder cancer/CAD for bladder cancer/images/'\n",
    "\n",
    "# Path to bladder cancer test data\n",
    "test_data_path = '/Users/vladarozova/Dropbox/Bladder cancer/bladder_cancer_test_data/'\n",
    "\n",
    "# Path to labels for bladder cancer data\n",
    "labels_path = '/Users/vladarozova/Dropbox/Bladder cancer/CAD for bladder cancer/training set/labels/'\n",
    "\n",
    "# Path to ImageNet training data\n",
    "train_on_imagenet = '/Users/vladarozova/Dropbox/Bladder cancer/CAD for bladder cancer/imagenet/'\n",
    "\n",
    "# Path to ImageNet test data\n",
    "test_on_imagenet = '/Users/vladarozova/Dropbox/Bladder cancer/bladder_cancer/test_images/'\n",
    "\n",
    "# Path to weights of the pretrained VGG16 model\n",
    "weights_name = 'vgg16.npy'\n",
    "\n",
    "# Store current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Input dimensions for VGG16\n",
    "input_height = 224\n",
    "input_width = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ImageNet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = prep.load_images(train_on_imagenet, input_height, input_width, plot=True)\n",
    "\n",
    "y_train = np.zeros((4, 1000), dtype = np.float32)\n",
    "y_train[0, 355] = 1.0 # llama\n",
    "y_train[1, 292] = 1.0 # tiger\n",
    "y_train[2, 291] = 1.0 # lion\n",
    "y_train[3, 150] = 1.0 # sea lion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = prep.load_images(test_on_imagenet, input_height, input_width, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images and labels for whole image binary classification\n",
    "images, labels = prep.load_images_with_labels(data_path, labels_path, input_height, input_width)\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stratified training and validation sets\n",
    "X_train, y_train, X_val, y_val = prep.stratified_train_val(images, labels, train_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardize tha data using mean and sd of the training set\n",
    "#X_train_std, train_mean, train_sd = prep.standardize(X_train, training_set=True)\n",
    "#X_val_std = prep.standardize(X_val, train_mean, train_sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_kernel(layer_name, trainable):\n",
    "    \"\"\"Returns kernel values for a conv layer from the pretrained model\"\"\"\n",
    "    return tf.Variable(weights_dict[layer_name][0], name=\"kernel\", trainable=trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bias(layer_name, trainable):\n",
    "    \"\"\"Returns bias values for a conv layer from the pretrained model\"\"\"\n",
    "    return tf.Variable(weights_dict[layer_name][1], name=\"bias\", trainable=trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fc_weights(layer_name, trainable):\n",
    "    \"\"\"Returns weights for a fc layer from the pretrained model\"\"\" \n",
    "    return tf.Variable(weights_dict[layer_name][0], name=\"kernel\", trainable=trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weights(layer_name, idx, name, trainable):\n",
    "    \"\"\"Returns weights from the pretrained model\"\"\" \n",
    "    return tf.Variable(weights_dict[layer_name][idx], name=name, trainable=trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(x, n_neurons, layer_name, activation=tf.nn.relu):\n",
    "    with tf.variable_scope(layer_name):  \n",
    "        # Check if this is a new layer\n",
    "        if layer_name in new_layers:\n",
    "            return tf.layers.conv2d(x, n_neurons,  [3, 3], activation=activation)\n",
    "            \n",
    "        # Check if the layer is trained \n",
    "        trainable = False\n",
    "        if layer_name in train_layers:\n",
    "            trainable = True\n",
    "            \n",
    "        # Get kernel values from weights_dict for 'layer_name'\n",
    "        kernel = get_kernel(layer_name, trainable)\n",
    "\n",
    "        # Get bias values from weights_dict for 'layer_name'\n",
    "        bias = get_bias(layer_name, trainable)\n",
    "\n",
    "        # Convolve input with preinitialized kernel\n",
    "        conv = tf.nn.conv2d(x, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "        # Add bias\n",
    "        layer = tf.nn.bias_add(conv, bias)\n",
    "        \n",
    "        # Apply activation\n",
    "        if activation is not None:\n",
    "            return activation(layer)\n",
    "        else:\n",
    "            return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool(x, layer_name):\n",
    "    \"\"\"Returns the feature map downsampled by 2x\"\"\"\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_layer(x, n_neurons, layer_name, activation=tf.nn.relu):\n",
    "    with tf.variable_scope(layer_name):\n",
    "        # Check if this is a new layer\n",
    "        if layer_name in new_layers:\n",
    "            return tf.layers.dense(x, n_neurons, activation=activation)\n",
    "        \n",
    "        # Check if the layer is trained\n",
    "        trainable=False\n",
    "        if layer_name in train_layers:\n",
    "            trainable=True\n",
    "            \n",
    "        # Get weights from weights_dict for 'layer_name'\n",
    "        weights = get_fc_weights(layer_name, trainable)\n",
    "\n",
    "        # Get bias values from weights_dict for 'layer_name'\n",
    "        bias = get_bias(layer_name, trainable)\n",
    "\n",
    "        # Flatten the input\n",
    "        shape = x.get_shape()\n",
    "        new_shape = reduce(lambda x, y: x*y, shape[1:])\n",
    "        x_flat = tf.reshape(x, [-1, new_shape])\n",
    "\n",
    "        # Multiply input by weights\n",
    "        fc = tf.matmul(x_flat, weights)\n",
    "\n",
    "        # Add bias\n",
    "        layer = tf.nn.bias_add(fc, bias, name='layer')\n",
    "\n",
    "        # Apply activation\n",
    "        if activation is not None:\n",
    "            return activation(layer)\n",
    "        else:\n",
    "            return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg16(x, n_classes, input_height, input_width):\n",
    "    with tf.variable_scope(\"VGG16\") as scope:\n",
    "        # Reshape the images to feed to VGG16 \n",
    "        x_input = tf.reshape(x, [-1, input_height, input_width, 3])\n",
    "        \n",
    "        # Block 1 consists of 2 convolutional layers followed by max pool\n",
    "        with tf.variable_scope(\"Block_1\") as scope:\n",
    "            conv1_1 = conv_layer(x_input, 64, 'conv1_1')\n",
    "            conv1_2 = conv_layer(conv1_1, 64, 'conv1_2')\n",
    "            pool1 = max_pool(conv1_2, 'pool1')\n",
    "        \n",
    "        # Block 2 consists of 2 convolutional layers followed by max pool\n",
    "        with tf.variable_scope(\"Block_2\") as scope:\n",
    "            conv2_1 = conv_layer(pool1, 128, 'conv2_1')\n",
    "            conv2_2 = conv_layer(conv2_1, 128, 'conv2_2')\n",
    "            pool2 = max_pool(conv2_2, 'pool2')\n",
    "        \n",
    "        # Block 3 consists of 3 convolutional layers followed by max pool\n",
    "        with tf.variable_scope(\"Block_3\") as scope:\n",
    "            conv3_1 = conv_layer(pool2, 256, 'conv3_1')\n",
    "            conv3_2 = conv_layer(conv3_1, 256, 'conv3_2')\n",
    "            conv3_3 = conv_layer(conv3_2, 256, 'conv3_3')\n",
    "            pool3 = max_pool(conv3_3, 'pool3')\n",
    "            \n",
    "        # Block 4 consists of 3 convolutional layers followed by max pool\n",
    "        with tf.variable_scope(\"Block_4\") as scope:\n",
    "            conv4_1 = conv_layer(pool3, 512, 'conv4_1')\n",
    "            conv4_2 = conv_layer(conv4_1, 512, 'conv4_2')\n",
    "            conv4_3 = conv_layer(conv4_2, 512, 'conv4_3')\n",
    "            pool4 = max_pool(conv4_3, 'pool4')\n",
    "            \n",
    "        # Block 5 consists of 3 convolutional layers followed by max pool\n",
    "        with tf.variable_scope(\"Block_5\") as scope:\n",
    "            conv5_1 = conv_layer(pool4, 512, 'conv5_1')\n",
    "            conv5_2 = conv_layer(conv5_1, 512, 'conv5_2')\n",
    "            conv5_3 = conv_layer(conv5_2, 512, 'conv5_3')\n",
    "            pool5 = max_pool(conv5_3, 'pool5')\n",
    "        \n",
    "        # First FC layer + ReLU\n",
    "        with tf.variable_scope(\"FC_layer_6\") as scope:\n",
    "            fc6 = fc_layer(pool5, 4096, 'fc6')\n",
    "        \n",
    "        # Second FC layer + ReLU\n",
    "        with tf.variable_scope(\"FC_layer_7\") as scope:\n",
    "            fc7 = fc_layer(fc6, 4096, 'fc7')\n",
    "            \n",
    "        # Third FC layer from original VGG16\n",
    "        #with tf.variable_scope(\"FC_layer_8\") as scope:\n",
    "        #    fc8 = fc_layer(fc7, 1000, 'fc8', , activation=None)\n",
    "            \n",
    "        # Third FC layer: map the 4096 features to 1 class for a binary classificator\n",
    "        # Do not apply activation, the output will be passed to tf.losses.sigmoid_cross_entropy\n",
    "        with tf.variable_scope(\"FC_layer_8\") as scope:\n",
    "            fc8 = fc_layer(fc7, n_classes, 'fc8', activation=None)   \n",
    "            \n",
    "    return fc8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    \"\"\"\n",
    "    This function resets the currrent TF graph\n",
    "    to avoid duplicate nodes\n",
    "    \"\"\"\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_dir(prefix=\"\"):\n",
    "    \"\"\"\n",
    "    This function creates a folder for TF logs\n",
    "    \"\"\"\n",
    "    now = datetime.now().strftime(\"%H%M%S%d%m%Y\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_with_reps(X, y, batch_size):\n",
    "    \"\"\"\n",
    "    This function extracts a new batch with repetiotions\n",
    "    \"\"\"\n",
    "    rnd_ind = np.random.randint(0, len(X), batch_size)\n",
    "    X_batch = X[rnd_ind, :, :, :]\n",
    "    y_batch = y[rnd_ind]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def labels_to_onehot(labels, num_classes):\n",
    "    \"\"\"\n",
    "    This function converts labels to one-hot vectors\n",
    "    \"\"\"\n",
    "    one_hot_labels = np.zeros((len(labels), num_classes))\n",
    "    for i in range(len(labels)):\n",
    "        one_hot_labels[i][labels[i].astype(np.int32)] = 1\n",
    "    return one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set current directory\n",
    "os.chdir(current_dir)\n",
    "\n",
    "# Reset graph\n",
    "reset_graph()\n",
    "\n",
    "# Learning params\n",
    "learning_rate = 0.01\n",
    "n_classes = 2\n",
    "n_epochs = 10\n",
    "batch_size = 10\n",
    "\n",
    "# Network params\n",
    "dropout_rate = 0.5\n",
    "\n",
    "# Layers \n",
    "new_layers = ['fc8']\n",
    "train_layers = new_layers #['fc7'] + new_layers\n",
    "\n",
    "# Load weights\n",
    "weights_dict = np.load(weights_name, encoding='latin1').item()\n",
    "\n",
    "# Path for tf.summary.FileWriter\n",
    "filewriter_path = log_dir(\"VGG16\")\n",
    "\n",
    "# Path to store checkpoints\n",
    "checkpoint_path = \"/tmp/vgg16/\"\n",
    "final_model_path = os.path.join(checkpoint_path, 'vgg16_final_model.ckpt')\n",
    "\n",
    "# Create parent path if it doesn't exist\n",
    "if not os.path.isdir(checkpoint_path): \n",
    "    os.mkdir(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF placeholders for graph input and output\n",
    "X = tf.placeholder(tf.float32, shape=[None, input_height, input_width, 3], name=\"X\")\n",
    "y = tf.placeholder(tf.float32, [None, n_classes], name=\"y\")\n",
    "\n",
    "# Output of the model\n",
    "logits = vgg16(X, n_classes, input_height, input_width)\n",
    "\n",
    "# Apply sigmoid function to obtain probabilities\n",
    "y_proba = tf.nn.softmax(logits, name=\"probabilities\")\n",
    "\n",
    "# Round to obtain predictions\n",
    "y_pred = tf.round(y_proba, name=\"predictions\")\n",
    "\n",
    "# Define the loss function\n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=logits))\n",
    "\n",
    "# List of trainable variables\n",
    "train_vars = [v for v in tf.trainable_variables() if v.name.split('/')[2] in train_layers]\n",
    "    \n",
    "# Define the training op\n",
    "with tf.name_scope(\"train\"):\n",
    "    # Get gradients of variables that will be visualised in TensorBoard\n",
    "    gradients = tf.gradients(loss, train_vars)\n",
    "    gradients = list(zip(gradients, train_vars))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    #training_op = optimizer.minimize(loss, var_list=check_vars)\n",
    "    \n",
    "\"\"\"# Define performance metrics and operations\n",
    "with tf.name_scope(\"eval\"):\n",
    "    # Accuracy\n",
    "    accuracy, op1 = tf.metrics.accuracy(tf.argmax(y, 1), tf.argmax(y_conv, 1), name=\"accuracy\")\n",
    "    \n",
    "    # Precision\n",
    "    precision, op2 = tf.metrics.precision(tf.argmax(y, 1), tf.argmax(y_conv, 1), name=\"precision\")\n",
    "    \n",
    "    # Recall\n",
    "    recall, op3 = tf.metrics.recall(tf.argmax(y, 1), tf.argmax(y_conv, 1), name=\"recall\")\n",
    "    \n",
    "    # F-score\n",
    "    fscore = tf.cast(2 * precision * recall / (precision + recall), tf.float32, name=\"fscore\")\"\"\"\n",
    "    \n",
    "# Where to add summary\n",
    "with tf.name_scope(\"summary\") as scope:   \n",
    "    # Add gradients to summary  \n",
    "    for gradient, var in gradients:\n",
    "        tf.summary.histogram(var.name + '/gradient', gradient)\n",
    "        \n",
    "    # Add the variables we train to the summary  \n",
    "    #for var in train_vars:\n",
    "    for var in train_vars:\n",
    "        tf.summary.histogram(var.name, var)\n",
    "        \n",
    "    # Add the loss to the  summary\n",
    "    loss_summary = tf.summary.scalar('Cross_entropy', loss)\n",
    "      \n",
    "    # Add the accuracy to the summary\n",
    "    #tf.summary.scalar('Accuracy', accuracy)\n",
    "      \n",
    "    # Add the fscore to the summary\n",
    "    #tf.summary.scalar('Fscore', fscore)\n",
    "\n",
    "    # Merge all summaries together\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    \n",
    "# Initialize the FileWriter\n",
    "writer = tf.summary.FileWriter(filewriter_path)\n",
    "\n",
    "# Global and local variables initializers\n",
    "global_init = tf.global_variables_initializer()\n",
    "local_init = tf.local_variables_initializer()\n",
    "\n",
    "# Initialize a saver for store model checkpoints\n",
    "#saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for var in tf.trainable_variables():\n",
    "    print(var.name, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for var in check_vars:\n",
    "    print(var.name, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create one-hot labels\n",
    "y_train_onehot = labels_to_onehot(y_train, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of training/validation steps per epoch\n",
    "train_batches_per_epoch = np.floor(len(X_train) / batch_size).astype(np.int16)\n",
    "val_batches_per_epoch = np.floor(len(X_val) / batch_size).astype(np.int16)\n",
    "print(train_batches_per_epoch)\n",
    "print(val_batches_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Tensorflow session\n",
    "with tf.Session() as sess:\n",
    " \n",
    "    # Initialize all variables\n",
    "    sess.run([global_init, local_init])\n",
    "  \n",
    "    # Add the model graph to TensorBoard\n",
    "    writer.add_graph(sess.graph)\n",
    "  \n",
    "    print(\"{} Start training...\".format(datetime.now()))\n",
    "    print(\"{} Open Tensorboard at --logdir {}\".format(datetime.now(), filewriter_path))\n",
    "  \n",
    "    # Loop over number of epochs\n",
    "    for epoch in range(n_epochs):    \n",
    "        print(\"{} Epoch number: {}\".format(datetime.now(), epoch + 1))\n",
    "        \n",
    "        for batch_index in range(train_batches_per_epoch):\n",
    "            # Get a new batch of images and labels\n",
    "            X_batch, y_batch = batch_with_reps(X_train, y_train_onehot, batch_size)\n",
    "            feed_dict = {X: X_batch, y: y_batch}\n",
    "            \n",
    "            # Run the training op\n",
    "            sess.run(training_op, feed_dict=feed_dict)\n",
    "            \n",
    "            # Check the performance every 5 batches\n",
    "            if batch_index % 5 == 0:\n",
    "                feed_dict={X: X_batch, y: y_batch}\n",
    "                step = epoch * train_batches_per_epoch + batch_index\n",
    "                \n",
    "                # Update ops to correctly calculate tf_metrics\n",
    "                #sess.run([op1, op2, op3], feed_dict=feed_dict)\n",
    "                \n",
    "                # Calculate tf_metrics and write the summary\n",
    "                summary_str = sess.run(merged_summary, feed_dict=feed_dict)\n",
    "                writer.add_summary(summary_str, step)\n",
    "                \n",
    "                # Flush the event file to disk\n",
    "                writer.flush()\n",
    "        \"\"\"\n",
    "        # Validate the model on the validation set using batches after each epoch\n",
    "        print(\"{} Start validation\".format(datetime.now()))\n",
    "        val_acc = 0.\n",
    "        val_count = 0\n",
    "        for _ in range(val_batches_per_epoch):\n",
    "            X_batch, y_batch = batch_with_reps(X_val_std, y_val_onehot, batch_size)\n",
    "            acc = sess.run(accuracy, feed_dict={X: X_batch, y: y_batch, keep_prob: 1.})\n",
    "            val_acc += acc\n",
    "            val_count += 1\n",
    "        val_acc /= val_count\n",
    "        print(\"{} Validation Accuracy = {:.4f}\".format(datetime.now(), val_acc))\n",
    "        \"\"\"\n",
    "        # Save checkpoint of the model\n",
    "        print(\"{} Saving checkpoint of model...\".format(datetime.now()))  \n",
    "        \n",
    "        #checkpoint_name = os.path.join(checkpoint_path, 'model_epoch'+str(epoch+1)+'.ckpt')\n",
    "        #saver.save(sess, checkpoint_name)  \n",
    "        \n",
    "        #print(\"{} Model checkpoint saved at {}\".format(datetime.now(), checkpoint_name))\n",
    "    \n",
    "    # Save the final model\n",
    "    print(\"{} Saving the final model...\".format(datetime.now()))\n",
    "    \n",
    "    #saver.save(sess, final_model_path)\n",
    "    \n",
    "    print(\"{} Final model saved at {}\".format(datetime.now(), final_model_path))\n",
    "    \n",
    "    # Isolate the variables stored behind the scenes by the metric operation\n",
    "    #stream_vars = [i for i in tf.local_variables() if 'fscore' in i.name] # variables associated with F score\n",
    "    \n",
    "    # Initialize/reset the selected running variables\n",
    "    #stream_vars_init = tf.variables_initializer(var_list=stream_vars)\n",
    "    #sess.run(stream_vars_init)\n",
    "    \n",
    "    # Validate the final model on the entire validation set\n",
    "    proba = sess.run(y_proba, feed_dict={X: X_val})\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba[:,1] >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = (proba[:,1] >= 0.5)\n",
    "print(\"Total:\", pred.size)\n",
    "print(\"Positive:\", sum(pred))\n",
    "print(\"Condition positive\", sum(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "print(\"Precision:\", precision_score(y_val, pred))\n",
    "print(\"Recall:\", recall_score(y_val, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_val, pred, labels = [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_val, pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(y_val, proba[:,1])\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Posiitve Rate')\n",
    "    \n",
    "plot_roc_curve(fpr, tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test images from ImageNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from caffe_classes import class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_images = prep.load_imagenet_images(test_on_imagenet, input_height, input_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the pretrained model on ImageNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Add the model graph to TensorBoard\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    # Create figure handle\n",
    "    fig2 = plt.figure(figsize=(15,6))\n",
    "    \n",
    "    # Loop over all images\n",
    "    for i, image in enumerate(test_images):\n",
    "        \n",
    "        img = image.reshape((1, input_height, input_width, 3))\n",
    "        \n",
    "        # Run the session and calculate the class probability\n",
    "        proba = sess.run(y_proba, feed_dict={X: img})\n",
    "        \n",
    "        # Get the class name of the class with the highest probability\n",
    "        class_name = class_names[np.argmax(proba)]\n",
    "        \n",
    "        # Plot image with class name and prob in the title\n",
    "        #fig2.add_subplot(1,3,i+1)\n",
    "        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(\"Class: \" + class_name + \", probability: %.4f\" %proba[0,np.argmax(proba)])\n",
    "        plt.axis('off')\n",
    "        \n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
