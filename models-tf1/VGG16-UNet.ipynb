{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import glob\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "import preprocess_data as prep\n",
    "from caffe_classes import class_names\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Path to the project\n",
    "project_path = '/Users/vladarozova/Dropbox/Bladder cancer/CAD/'\n",
    "\n",
    "# Path to bladder cancer training data\n",
    "data_path = os.path.join(project_path, 'images/')\n",
    "\n",
    "# Path to bladder cancer test data\n",
    "test_data_path = os.path.join(project_path, 'test set/')\n",
    "\n",
    "# Path to labels for bladder cancer data\n",
    "labels_path = os.path.join(project_path, 'training set/labels/')\n",
    "\n",
    "# Path to masks for bladder cancer data\n",
    "masks_path = os.path.join(project_path, 'training set/masks/')\n",
    "\n",
    "# Path to ImageNet training data\n",
    "train_on_imagenet = os.path.join(project_path, 'bladder_cancer_scripts/various models/imagenet/')\n",
    "\n",
    "# Path to ImageNet test data\n",
    "test_on_imagenet = os.path.join(project_path, 'bladder_cancer_scripts/various models/imagenet/')\n",
    "\n",
    "# Path to weights of the pretrained VGG16 model\n",
    "weights_name = os.path.join(project_path, 'weights/vgg16.npy')\n",
    "\n",
    "# Store current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Input dimensions for VGG16\n",
    "input_height = 224\n",
    "input_width = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load bladder cancer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load images and corresponding masks for UNet\n",
    "images, masks = prep.load_images_with_masks(data_path, masks_path, input_height, input_width, binary=True)\n",
    "assert len(images) == len(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load images and labels for whole-image binary classification\n",
    "#images, labels = prep.load_images_with_labels(data_path, labels_path, input_height, input_width)\n",
    "#assert images.shape[0] == labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (100, 224, 224, 3) (100, 224, 224)\n",
      "Validation set: (25, 224, 224, 3) (25, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "# Create stratified training and validation sets\n",
    "X_train, y_train, X_val, y_val = prep.split_train_val(images, masks, train_ratio=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_kernel(layer_name, trainable):\n",
    "    \"\"\"Returns kernel values for a conv layer from the pretrained model\"\"\"\n",
    "    return tf.Variable(weights_dict[layer_name][0], name=\"kernel\", trainable=trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bias(layer_name, trainable):\n",
    "    \"\"\"Returns bias values for a conv layer from the pretrained model\"\"\"\n",
    "    return tf.Variable(weights_dict[layer_name][1], name=\"bias\", trainable=trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fc_weights(layer_name, trainable):\n",
    "    \"\"\"Returns weights for a fc layer from the pretrained model\"\"\" \n",
    "    return tf.Variable(weights_dict[layer_name][0], name=\"kernel\", trainable=trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weights(layer_name, idx, name, trainable):\n",
    "    \"\"\"Returns weights from the pretrained model\"\"\" \n",
    "    return tf.Variable(weights_dict[layer_name][idx], name=name, trainable=trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(x, n_neurons, layer_name, kernel_size=[3, 3], padding='SAME', activation=tf.nn.relu):\n",
    "    with tf.variable_scope(layer_name):  \n",
    "        # Check if this is a new layer\n",
    "        if layer_name in new_layers:                \n",
    "            return tf.layers.conv2d(x, n_neurons, kernel_size=kernel_size, padding=padding, activation=activation)\n",
    "            \n",
    "        # Check if the layer is trained \n",
    "        trainable = False\n",
    "        if layer_name in train_layers:\n",
    "            trainable = True\n",
    "            \n",
    "        # Get kernel values from weights_dict for 'layer_name'\n",
    "        kernel = get_kernel(layer_name, trainable)\n",
    "\n",
    "        # Get bias values from weights_dict for 'layer_name'\n",
    "        bias = get_bias(layer_name, trainable)\n",
    "\n",
    "        # Convolve input with preinitialized kernel\n",
    "        conv = tf.nn.conv2d(x, kernel, strides=[1, 1, 1, 1], padding=padding)\n",
    "\n",
    "        # Add bias\n",
    "        layer = tf.nn.bias_add(conv, bias)\n",
    "        \n",
    "        # Apply activation\n",
    "        if activation is not None:\n",
    "            return activation(layer)\n",
    "        else:\n",
    "            return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transpConv_layer(x, n_neurons, layer_name, activation=tf.nn.relu):\n",
    "    with tf.variable_scope(layer_name):  \n",
    "        return tf.layers.conv2d_transpose(x, n_neurons, [3, 3], strides=(2, 2), padding='SAME', activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool(x, layer_name):\n",
    "    \"\"\"Returns the feature map downsampled by 2x\"\"\"\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_layer(x, n_neurons, layer_name, activation=tf.nn.relu):\n",
    "    with tf.variable_scope(layer_name):\n",
    "        # Check if this is a new layer\n",
    "        if layer_name in new_layers:\n",
    "            return tf.layers.dense(x, n_neurons, activation=activation)\n",
    "        \n",
    "        # Check if the layer is trained\n",
    "        trainable=False\n",
    "        if layer_name in train_layers:\n",
    "            trainable=True\n",
    "            \n",
    "        # Get weights from weights_dict for 'layer_name'\n",
    "        weights = get_fc_weights(layer_name, trainable)\n",
    "\n",
    "        # Get bias values from weights_dict for 'layer_name'\n",
    "        bias = get_bias(layer_name, trainable)\n",
    "\n",
    "        # Flatten the input\n",
    "        shape = x.get_shape()\n",
    "        new_shape = reduce(lambda x, y: x*y, shape[1:])\n",
    "        x_flat = tf.reshape(x, [-1, new_shape])\n",
    "\n",
    "        # Multiply input by weights\n",
    "        fc = tf.matmul(x_flat, weights)\n",
    "\n",
    "        # Add bias\n",
    "        layer = tf.nn.bias_add(fc, bias, name='layer')\n",
    "\n",
    "        # Apply activation\n",
    "        if activation is not None:\n",
    "            return activation(layer)\n",
    "        else:\n",
    "            return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg16_unet(x, n_classes, input_height, input_width):\n",
    "    with tf.variable_scope(\"VGG16_UNet\") as scope:\n",
    "        # Reshape the images to feed to VGG16_UNet \n",
    "        x_input = tf.reshape(x, [-1, input_height, input_width, 3])\n",
    "        \n",
    "        # Block 1 consists of 2 convolutional layers followed by max pool\n",
    "        with tf.variable_scope(\"Block_1\") as scope:\n",
    "            conv1_1 = conv_layer(x_input, 64, 'conv1_1')\n",
    "            conv1_2 = conv_layer(conv1_1, 64, 'conv1_2')\n",
    "            pool1 = max_pool(conv1_2, 'pool1')\n",
    "            \n",
    "            print(\"conv1_1 shape: {}\".format(conv1_1.shape))\n",
    "            print(\"conv1_2 shape: {}\".format(conv1_2.shape))\n",
    "            print(\"pool1 shape: {}\".format(pool1.shape))\n",
    "        \n",
    "        # Block 2 consists of 2 convolutional layers followed by max pool\n",
    "        with tf.variable_scope(\"Block_2\") as scope:\n",
    "            conv2_1 = conv_layer(pool1, 128, 'conv2_1')\n",
    "            conv2_2 = conv_layer(conv2_1, 128, 'conv2_2')\n",
    "            pool2 = max_pool(conv2_2, 'pool2')\n",
    "            \n",
    "            print(\"conv2_1 shape: {}\".format(conv2_1.shape))\n",
    "            print(\"conv2_2 shape: {}\".format(conv2_2.shape))\n",
    "            print(\"pool2 shape: {}\".format(pool2.shape))\n",
    "        \n",
    "        # Block 3 consists of 3 convolutional layers followed by max pool\n",
    "        with tf.variable_scope(\"Block_3\") as scope:\n",
    "            conv3_1 = conv_layer(pool2, 256, 'conv3_1')\n",
    "            conv3_2 = conv_layer(conv3_1, 256, 'conv3_2')\n",
    "            conv3_3 = conv_layer(conv3_2, 256, 'conv3_3')\n",
    "            pool3 = max_pool(conv3_3, 'pool3')\n",
    "            \n",
    "            print(\"conv3_1 shape: {}\".format(conv3_1.shape))\n",
    "            print(\"conv3_2 shape: {}\".format(conv3_2.shape))\n",
    "            print(\"conv3_3 shape: {}\".format(conv3_3.shape))\n",
    "            print(\"pool3 shape: {}\".format(pool3.shape))\n",
    "            \n",
    "        # Block 4 consists of 3 convolutional layers followed by max pool\n",
    "        with tf.variable_scope(\"Block_4\") as scope:\n",
    "            conv4_1 = conv_layer(pool3, 512, 'conv4_1')\n",
    "            conv4_2 = conv_layer(conv4_1, 512, 'conv4_2')\n",
    "            conv4_3 = conv_layer(conv4_2, 512, 'conv4_3')\n",
    "            pool4 = max_pool(conv4_3, 'pool4')\n",
    "            \n",
    "            print(\"conv4_1 shape: {}\".format(conv4_1.shape))\n",
    "            print(\"conv4_2 shape: {}\".format(conv4_2.shape))\n",
    "            print(\"conv4_3 shape: {}\".format(conv4_3.shape))\n",
    "            print(\"pool4 shape: {}\".format(pool4.shape))\n",
    "            \n",
    "        # Block 5 consists of 3 convolutional layers followed by max pool\n",
    "        with tf.variable_scope(\"Block_5\") as scope:\n",
    "            conv5_1 = conv_layer(pool4, 512, 'conv5_1')\n",
    "            conv5_2 = conv_layer(conv5_1, 512, 'conv5_2')\n",
    "            conv5_3 = conv_layer(conv5_2, 512, 'conv5_3')\n",
    "            pool5 = max_pool(conv5_3, 'pool5')\n",
    "            \n",
    "            print(\"conv5_1 shape: {}\".format(conv5_1.shape))\n",
    "            print(\"conv5_2 shape: {}\".format(conv5_2.shape))\n",
    "            print(\"conv5_3 shape: {}\".format(conv5_3.shape))\n",
    "            print(\"pool5 shape: {}\".format(pool5.shape))\n",
    "            \n",
    "        # One convolution at the bottom of the UNet\n",
    "        with tf.variable_scope(\"Bottom\") as scope:\n",
    "            bottom_conv = conv_layer(pool5, 1024, 'bottom_conv')\n",
    "            \n",
    "            print(\"bottom_conv shape: {}\".format(bottom_conv.shape))\n",
    "        \n",
    "        # Block 6 consists a transposed convolution and concatenation\n",
    "        # followed by 3 convolutional layers\n",
    "        with tf.variable_scope(\"Block_6\") as scope:\n",
    "            print(\"Block 6\")\n",
    "            transpConv6 = transpConv_layer(bottom_conv, 512, 'transpConv6')\n",
    "            \n",
    "            print(\"transpConv6 shape: {}\".format(transpConv6.shape))\n",
    "            print(\"conv5_3 shape: {}\".format(conv5_3.shape))\n",
    "            print(\"Concatenation...\")\n",
    "            \n",
    "            concat6 = tf.concat([transpConv6, conv5_3], axis=3) \n",
    "            print(\"concat6 shape: {}\".format(concat6.shape))\n",
    "            \n",
    "            conv6_1 = conv_layer(concat6, 512, 'conv6_1')\n",
    "            conv6_2 = conv_layer(conv6_1, 512, 'conv6_2')\n",
    "            conv6_3 = conv_layer(conv6_2, 512, 'conv6_3')\n",
    "            \n",
    "            print(\"conv6_1 shape: {}\".format(conv6_1.shape))\n",
    "            print(\"conv6_2 shape: {}\".format(conv6_2.shape))\n",
    "            print(\"conv6_3 shape: {}\".format(conv6_3.shape))\n",
    "            \n",
    "        # Block 7 consists a transposed convolution and concatenation\n",
    "        # followed by 3 convolutional layers\n",
    "        with tf.variable_scope(\"Block_7\") as scope:\n",
    "            print(\"Block 7\")\n",
    "            transpConv7 = transpConv_layer(conv6_3, 512, 'transpConv6')\n",
    "            \n",
    "            print(\"transpConv7 shape: {}\".format(transpConv7.shape))\n",
    "            print(\"conv4_3 shape: {}\".format(conv4_3.shape))\n",
    "            print(\"Concatenation...\")\n",
    "            \n",
    "            concat7 = tf.concat([transpConv7, conv4_3], axis=3) \n",
    "            print(\"concat7 shape: {}\".format(concat7.shape))\n",
    "            \n",
    "            conv7_1 = conv_layer(concat7, 512, 'conv7_1')\n",
    "            conv7_2 = conv_layer(conv7_1, 512, 'conv7_2')\n",
    "            conv7_3 = conv_layer(conv7_2, 512, 'conv7_3')\n",
    "            \n",
    "            print(\"conv7_1 shape: {}\".format(conv7_1.shape))\n",
    "            print(\"conv7_2 shape: {}\".format(conv7_2.shape))\n",
    "            print(\"conv7_3 shape: {}\".format(conv7_3.shape))\n",
    "            \n",
    "        # Block 8 consists a transposed convolution and concatenation\n",
    "        # followed by 3 convolutional layers\n",
    "        with tf.variable_scope(\"Block_8\") as scope:\n",
    "            print(\"Block 8\")\n",
    "            transpConv8 = transpConv_layer(conv7_3, 256, 'transpConv8')\n",
    "            \n",
    "            print(\"transpConv8 shape: {}\".format(transpConv8.shape))\n",
    "            print(\"conv3_3 shape: {}\".format(conv3_3.shape))\n",
    "            print(\"Concatenation...\")\n",
    "            \n",
    "            concat8 = tf.concat([transpConv8, conv3_3], axis=3) \n",
    "            print(\"concat8 shape: {}\".format(concat8.shape))\n",
    "            \n",
    "            conv8_1 = conv_layer(concat8, 256, 'conv8_1')\n",
    "            conv8_2 = conv_layer(conv8_1, 256, 'conv8_2')\n",
    "            conv8_3 = conv_layer(conv8_2, 256, 'conv8_3')\n",
    "            \n",
    "            print(\"conv8_1 shape: {}\".format(conv8_1.shape))\n",
    "            print(\"conv8_2 shape: {}\".format(conv8_2.shape))\n",
    "            print(\"conv8_3 shape: {}\".format(conv8_3.shape))\n",
    "            \n",
    "        # Block 9 consists a transposed convolution and concatenation\n",
    "        # followed by 2 convolutional layers\n",
    "        with tf.variable_scope(\"Block_9\") as scope:\n",
    "            print(\"Block 9\")\n",
    "            transpConv9 = transpConv_layer(conv8_3, 128, 'transpConv9')\n",
    "            \n",
    "            print(\"transpConv9 shape: {}\".format(transpConv9.shape))\n",
    "            print(\"conv2_2 shape: {}\".format(conv2_2.shape))\n",
    "            print(\"Concatenation...\")\n",
    "            \n",
    "            concat9 = tf.concat([transpConv9, conv2_2], axis=3) \n",
    "            print(\"concat9 shape: {}\".format(concat9.shape))\n",
    "            \n",
    "            conv9_1 = conv_layer(concat9, 128, 'conv9_1')\n",
    "            conv9_2 = conv_layer(conv9_1, 128, 'conv9_2')\n",
    "            \n",
    "            print(\"conv9_1 shape: {}\".format(conv9_1.shape))\n",
    "            print(\"conv9_2 shape: {}\".format(conv9_2.shape))\n",
    "            \n",
    "        # Block 10 consists a transposed convolution and concatenation\n",
    "        # followed by 1 convolutional layers\n",
    "        with tf.variable_scope(\"Block_10\") as scope:\n",
    "            print(\"Block 10\")\n",
    "            transpConv10 = transpConv_layer(conv9_2, 64, 'transpConv10')\n",
    "            \n",
    "            print(\"transpConv10 shape: {}\".format(transpConv10.shape))\n",
    "            print(\"conv1_2 shape: {}\".format(conv1_2.shape))\n",
    "            print(\"Concatenation...\")\n",
    "            \n",
    "            concat10 = tf.concat([transpConv10, conv1_2], axis=3) \n",
    "            print(\"concat10 shape: {}\".format(concat10.shape))\n",
    "            \n",
    "            conv10_1 = conv_layer(concat10, 64, 'conv10_1')\n",
    "            \n",
    "            print(\"conv10_1 shape: {}\".format(conv10_1.shape))\n",
    "        \n",
    "        # The last layer is a 1x1 convolution that maps each\n",
    "        # 64-component feature vector to the desired number of classes\n",
    "        with tf.variable_scope(\"Final_layer\") as scope:\n",
    "            print(\"Final layer\")\n",
    "            final = conv_layer(conv10_1, n_classes, 'final', kernel_size=[1, 1], padding='VALID', activation=None)\n",
    "            \n",
    "            print(\"final shape: {}\".format(final.shape))\n",
    "            \n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    \"\"\"\n",
    "    This function resets the currrent TF graph\n",
    "    to avoid duplicate nodes\n",
    "    \"\"\"\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_dir(prefix=\"\"):\n",
    "    \"\"\"\n",
    "    This function creates a folder for TF logs\n",
    "    \"\"\"\n",
    "    now = datetime.now().strftime(\"%H%M%S%d%m%Y\")\n",
    "    root_logdir = \"tf_logs_unet\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_with_reps(X, y, batch_size):\n",
    "    \"\"\"\n",
    "    This function extracts a new batch with repetiotions\n",
    "    \"\"\"\n",
    "    rnd_ind = np.random.randint(0, len(X), batch_size)\n",
    "    X_batch = X[rnd_ind, :, :, :]\n",
    "    y_batch = y[rnd_ind]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def labels_to_onehot(y, n_classes):\n",
    "    \"\"\"\n",
    "    This function converts labels to one-hot vectors\n",
    "    \"\"\"\n",
    "    not_y = (y + 1) % 2\n",
    "\n",
    "    one_hot = np.zeros((y.shape + (n_classes,)), dtype = np.float32)\n",
    "    one_hot[:,:,:,0] = not_y\n",
    "    one_hot[:,:,:,1] = y\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set current directory\n",
    "os.chdir(current_dir)\n",
    "\n",
    "# Reset graph\n",
    "reset_graph()\n",
    "\n",
    "# Learning params\n",
    "learning_rate = 0.01\n",
    "n_classes = 2\n",
    "n_epochs = 3\n",
    "batch_size = 1\n",
    "\n",
    "# Network params\n",
    "dropout_rate = 0.5\n",
    "\n",
    "# Layers \n",
    "new_layers = ['bottom_conv', \n",
    "              'transpConv6', 'conv6_1', 'conv6_2', 'conv6_3', \n",
    "              'transpConv7', 'conv7_1', 'conv7_2', 'conv7_3', \n",
    "              'transpConv8', 'conv8_1', 'conv8_2', 'conv8_3', \n",
    "              'transpConv9', 'conv9_1', 'conv9_2', \n",
    "              'transpConv10', 'conv10_1', 'final']\n",
    "train_layers = ''#new_layers + ['conv5_3'] +['conv5_2'] + ['conv5_1'] + ['conv4_3'] + ['conv4_2'] + ['conv4_1']\n",
    " \n",
    "# Load weights\n",
    "weights_dict = np.load(weights_name, encoding='latin1', allow_pickle = True).item()\n",
    "\n",
    "# Path for tf.summary.FileWriter\n",
    "filewriter_path = log_dir(\"VGG16_UNet\")\n",
    "\n",
    "# Path to store checkpoints\n",
    "checkpoint_path = \"/tmp/vgg16_unet/\"\n",
    "final_model_path = os.path.join(checkpoint_path, 'vgg16_unet_final_model.ckpt')\n",
    "\n",
    "# Create parent path if it doesn't exist\n",
    "if not os.path.isdir(checkpoint_path): \n",
    "    os.mkdir(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dice_loss(y_true, y_pred):\n",
    "    numerator = 2 * tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "    denominator = tf.reduce_sum(y_true + y_pred, axis=-1)\n",
    "    dice = 1 - (numerator + 1) / (denominator + 1)\n",
    "#     dice = tf.reduce_mean(dice)\n",
    "    \n",
    "    return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1_1 shape: (?, 224, 224, 64)\n",
      "conv1_2 shape: (?, 224, 224, 64)\n",
      "pool1 shape: (?, 112, 112, 64)\n",
      "conv2_1 shape: (?, 112, 112, 128)\n",
      "conv2_2 shape: (?, 112, 112, 128)\n",
      "pool2 shape: (?, 56, 56, 128)\n",
      "conv3_1 shape: (?, 56, 56, 256)\n",
      "conv3_2 shape: (?, 56, 56, 256)\n",
      "conv3_3 shape: (?, 56, 56, 256)\n",
      "pool3 shape: (?, 28, 28, 256)\n",
      "conv4_1 shape: (?, 28, 28, 512)\n",
      "conv4_2 shape: (?, 28, 28, 512)\n",
      "conv4_3 shape: (?, 28, 28, 512)\n",
      "pool4 shape: (?, 14, 14, 512)\n",
      "conv5_1 shape: (?, 14, 14, 512)\n",
      "conv5_2 shape: (?, 14, 14, 512)\n",
      "conv5_3 shape: (?, 14, 14, 512)\n",
      "pool5 shape: (?, 7, 7, 512)\n",
      "bottom_conv shape: (?, 7, 7, 1024)\n",
      "Block 6\n",
      "transpConv6 shape: (?, 14, 14, 512)\n",
      "conv5_3 shape: (?, 14, 14, 512)\n",
      "Concatenation...\n",
      "concat6 shape: (?, 14, 14, 1024)\n",
      "conv6_1 shape: (?, 14, 14, 512)\n",
      "conv6_2 shape: (?, 14, 14, 512)\n",
      "conv6_3 shape: (?, 14, 14, 512)\n",
      "Block 7\n",
      "transpConv7 shape: (?, 28, 28, 512)\n",
      "conv4_3 shape: (?, 28, 28, 512)\n",
      "Concatenation...\n",
      "concat7 shape: (?, 28, 28, 1024)\n",
      "conv7_1 shape: (?, 28, 28, 512)\n",
      "conv7_2 shape: (?, 28, 28, 512)\n",
      "conv7_3 shape: (?, 28, 28, 512)\n",
      "Block 8\n",
      "transpConv8 shape: (?, 56, 56, 256)\n",
      "conv3_3 shape: (?, 56, 56, 256)\n",
      "Concatenation...\n",
      "concat8 shape: (?, 56, 56, 512)\n",
      "conv8_1 shape: (?, 56, 56, 256)\n",
      "conv8_2 shape: (?, 56, 56, 256)\n",
      "conv8_3 shape: (?, 56, 56, 256)\n",
      "Block 9\n",
      "transpConv9 shape: (?, 112, 112, 128)\n",
      "conv2_2 shape: (?, 112, 112, 128)\n",
      "Concatenation...\n",
      "concat9 shape: (?, 112, 112, 256)\n",
      "conv9_1 shape: (?, 112, 112, 128)\n",
      "conv9_2 shape: (?, 112, 112, 128)\n",
      "Block 10\n",
      "transpConv10 shape: (?, 224, 224, 64)\n",
      "conv1_2 shape: (?, 224, 224, 64)\n",
      "Concatenation...\n",
      "concat10 shape: (?, 224, 224, 128)\n",
      "conv10_1 shape: (?, 224, 224, 64)\n",
      "Final layer\n",
      "final shape: (?, 224, 224, 2)\n",
      "logits shape: (?, 224, 224, 2)\n",
      "probabilities shape: (?, 224, 224, 2)\n",
      "predictions shape: (?, 224, 224, 2)\n",
      "y shape: (?, 224, 224, 2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'VGG16_UNet/Bottom/bottom_conv/conv2d/kernel:0' shape=(3, 3, 512, 1024) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Bottom/bottom_conv/conv2d/bias:0' shape=(1024,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_6/transpConv6/conv2d_transpose/kernel:0' shape=(3, 3, 512, 1024) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_6/transpConv6/conv2d_transpose/bias:0' shape=(512,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_6/conv6_1/conv2d/kernel:0' shape=(3, 3, 1024, 512) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_6/conv6_1/conv2d/bias:0' shape=(512,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_6/conv6_2/conv2d/kernel:0' shape=(3, 3, 512, 512) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_6/conv6_2/conv2d/bias:0' shape=(512,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_6/conv6_3/conv2d/kernel:0' shape=(3, 3, 512, 512) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_6/conv6_3/conv2d/bias:0' shape=(512,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_7/transpConv6/conv2d_transpose/kernel:0' shape=(3, 3, 512, 512) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_7/transpConv6/conv2d_transpose/bias:0' shape=(512,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_7/conv7_1/conv2d/kernel:0' shape=(3, 3, 1024, 512) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_7/conv7_1/conv2d/bias:0' shape=(512,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_7/conv7_2/conv2d/kernel:0' shape=(3, 3, 512, 512) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_7/conv7_2/conv2d/bias:0' shape=(512,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_7/conv7_3/conv2d/kernel:0' shape=(3, 3, 512, 512) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_7/conv7_3/conv2d/bias:0' shape=(512,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_8/transpConv8/conv2d_transpose/kernel:0' shape=(3, 3, 256, 512) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_8/transpConv8/conv2d_transpose/bias:0' shape=(256,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_8/conv8_1/conv2d/kernel:0' shape=(3, 3, 512, 256) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_8/conv8_1/conv2d/bias:0' shape=(256,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_8/conv8_2/conv2d/kernel:0' shape=(3, 3, 256, 256) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_8/conv8_2/conv2d/bias:0' shape=(256,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_8/conv8_3/conv2d/kernel:0' shape=(3, 3, 256, 256) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_8/conv8_3/conv2d/bias:0' shape=(256,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_9/transpConv9/conv2d_transpose/kernel:0' shape=(3, 3, 128, 256) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_9/transpConv9/conv2d_transpose/bias:0' shape=(128,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_9/conv9_1/conv2d/kernel:0' shape=(3, 3, 256, 128) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_9/conv9_1/conv2d/bias:0' shape=(128,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_9/conv9_2/conv2d/kernel:0' shape=(3, 3, 128, 128) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_9/conv9_2/conv2d/bias:0' shape=(128,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_10/transpConv10/conv2d_transpose/kernel:0' shape=(3, 3, 64, 128) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_10/transpConv10/conv2d_transpose/bias:0' shape=(64,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_10/conv10_1/conv2d/kernel:0' shape=(3, 3, 128, 64) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_10/conv10_1/conv2d/bias:0' shape=(64,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Final_layer/final/conv2d/kernel:0' shape=(1, 1, 64, 2) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Final_layer/final/conv2d/bias:0' shape=(2,) dtype=float32_ref>\"] and loss Tensor(\"loss/sub:0\", shape=(?,), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-0df07cdea01c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# GraphKeys.TRAINABLE_VARIABLES collection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m#     assert tf.trainable_variables() == train_vars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mtraining_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# Define performance metrics and operations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    408\u001b[0m           \u001b[0;34m\"No gradients provided for any variable, check your graph for ops\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m           \u001b[0;34m\" that do not support gradients, between variables %s and loss %s.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m           ([str(v) for _, v in grads_and_vars], loss))\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'VGG16_UNet/Bottom/bottom_conv/conv2d/kernel:0' shape=(3, 3, 512, 1024) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Bottom/bottom_conv/conv2d/bias:0' shape=(1024,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_6/transpConv6/conv2d_transpose/kernel:0' shape=(3, 3, 512, 1024) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_6/transpConv6/conv2d_transpose/bias:0' shape=(512,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_6/conv6_1/conv2d/kernel:0' shape=(3, 3, 1024, 512) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_6/conv6_1/conv2d/bias:0' shape=(512,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_6/conv6_2/conv2d/kernel:0' shape=(3, 3, 512, 512) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_6/conv6_2/conv2d/bias:0' shape=(512,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_6/conv6_3/conv2d/kernel:0' shape=(3, 3, 512, 512) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_6/conv6_3/conv2d/bias:0' shape=(512,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_7/transpConv6/conv2d_transpose/kernel:0' shape=(3, 3, 512, 512) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_7/transpConv6/conv2d_transpose/bias:0' shape=(512,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_7/conv7_1/conv2d/kernel:0' shape=(3, 3, 1024, 512) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_7/conv7_1/conv2d/bias:0' shape=(512,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_7/conv7_2/conv2d/kernel:0' shape=(3, 3, 512, 512) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_7/conv7_2/conv2d/bias:0' shape=(512,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_7/conv7_3/conv2d/kernel:0' shape=(3, 3, 512, 512) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_7/conv7_3/conv2d/bias:0' shape=(512,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_8/transpConv8/conv2d_transpose/kernel:0' shape=(3, 3, 256, 512) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_8/transpConv8/conv2d_transpose/bias:0' shape=(256,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_8/conv8_1/conv2d/kernel:0' shape=(3, 3, 512, 256) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_8/conv8_1/conv2d/bias:0' shape=(256,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_8/conv8_2/conv2d/kernel:0' shape=(3, 3, 256, 256) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_8/conv8_2/conv2d/bias:0' shape=(256,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_8/conv8_3/conv2d/kernel:0' shape=(3, 3, 256, 256) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_8/conv8_3/conv2d/bias:0' shape=(256,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_9/transpConv9/conv2d_transpose/kernel:0' shape=(3, 3, 128, 256) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_9/transpConv9/conv2d_transpose/bias:0' shape=(128,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_9/conv9_1/conv2d/kernel:0' shape=(3, 3, 256, 128) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_9/conv9_1/conv2d/bias:0' shape=(128,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_9/conv9_2/conv2d/kernel:0' shape=(3, 3, 128, 128) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_9/conv9_2/conv2d/bias:0' shape=(128,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_10/transpConv10/conv2d_transpose/kernel:0' shape=(3, 3, 64, 128) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_10/transpConv10/conv2d_transpose/bias:0' shape=(64,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_10/conv10_1/conv2d/kernel:0' shape=(3, 3, 128, 64) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Block_10/conv10_1/conv2d/bias:0' shape=(64,) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Final_layer/final/conv2d/kernel:0' shape=(1, 1, 64, 2) dtype=float32_ref>\", \"<tf.Variable 'VGG16_UNet/Final_layer/final/conv2d/bias:0' shape=(2,) dtype=float32_ref>\"] and loss Tensor(\"loss/sub:0\", shape=(?,), dtype=float32)."
     ]
    }
   ],
   "source": [
    "# TF placeholders for graph input and output\n",
    "X = tf.placeholder(tf.float32, shape=[None, input_height, input_width, 3], name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=[None, input_height, input_width, n_classes], name=\"y\")\n",
    "\n",
    "# Output of the model\n",
    "logits = vgg16_unet(X, n_classes, input_height, input_width)\n",
    "print(\"logits shape: {}\".format(logits.shape))\n",
    "\n",
    "# Apply sigmoid function to obtain probabilities\n",
    "y_proba = tf.nn.softmax(logits, name=\"probabilities\") # by default axis=-1\n",
    "print(\"probabilities shape: {}\".format(y_proba.shape))\n",
    "\n",
    "# Round to obtain predictions\n",
    "y_pred = tf.round(y_proba, name=\"predictions\")\n",
    "print(\"predictions shape: {}\".format(y_pred.shape))\n",
    "\n",
    "print(\"y shape: {}\".format(y.shape))\n",
    "\n",
    "# Assert the output has the same shape as the input\n",
    "#assert logits.shape == y.shape\n",
    "\n",
    "if train_layers:\n",
    "    # List of trainable variables\n",
    "    train_vars = [v for v in tf.trainable_variables() if v.name.split('/')[2] in train_layers]\n",
    "    print(\"{} trainable variables:\".format(len(train_vars)))\n",
    "    for v in train_vars:\n",
    "        print(v.name)\n",
    "\n",
    "# Define the loss function\n",
    "with tf.name_scope(\"loss\"):\n",
    "#     y_flat = tf.reshape(y, [-1, n_classes])\n",
    "#     logits_flat = tf.reshape(logits, [-1, n_classes])\n",
    "#     loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_flat, logits=logits_flat))\n",
    "\n",
    "    y_flat = tf.reshape(y, [-1, 1])\n",
    "    pred_flat = tf.reshape(y_pred, [-1, 1])\n",
    "    loss = dice_loss(y_flat, pred_flat)\n",
    "\n",
    "    # TODO: add smoothing term\n",
    "    #smooth = tf.reduce_mean(tf.squared_difference(pred_maps[:, 0:height-1, :, :], pred_maps[:, 1:, :, :]))\n",
    "    #smooth = tf.constant(weight, dtype=tf.float32)*(smooth + tf.reduce_mean(tf.squared_difference(pred_maps[:, :, 0:width-1, :], pred_maps[:, :, 1:, :])))\n",
    "    #smoothed_loss = loss + smooth\n",
    "    \n",
    "# Define the training op\n",
    "with tf.name_scope(\"train\"):\n",
    "    if train_layers:\n",
    "        # Get gradients of variables that will be visualised in TensorBoard\n",
    "        gradients = tf.gradients(loss, train_vars)\n",
    "        gradients = list(zip(gradients, train_vars))\n",
    "    \n",
    "    # Create optimizer and apply Adam optimizer to the trainable variables\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    # By default gradients are computed using \n",
    "    # GraphKeys.TRAINABLE_VARIABLES collection\n",
    "#     assert tf.trainable_variables() == train_vars\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "# Define performance metrics and operations\n",
    "with tf.name_scope(\"eval\"):\n",
    "    # IoU\n",
    "    labels = tf.cast(y[:, :, :, 1], tf.int32)\n",
    "    predictions = tf.cast(y_pred[:, :, :, 1], tf.int32)\n",
    "    iou, op = tf.metrics.mean_iou(labels, predictions, n_classes, name=\"iou\")\n",
    "    \n",
    "    # Custom IoU from YPDL\n",
    "    meanIoU = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n",
    "    classIoU = []\n",
    "    pred_max = tf.reduce_max(y_proba, axis=3) #[, , ,]\n",
    "\n",
    "    for c in range(n_classes):\n",
    "        class_map = y_proba[:, :, :, c]\n",
    "        class_max = tf.equal(class_map, pred_max) # bool\n",
    "        gt_class = tf.cast(y[:, :, :, c], tf.bool)\n",
    "        Inter = tf.reduce_sum(tf.cast(tf.logical_and(gt_class, class_max), tf.float32), axis=[1, 2])\n",
    "        Union = tf.reduce_sum(tf.cast(tf.logical_or(gt_class, class_max), tf.float32), axis=[1, 2])\n",
    "        # check for validity\n",
    "        #tf.assert_greater_equal(Union, Inter)\n",
    "        #print \"Inter shape: {}\".format(Inter.shape)\n",
    "        #print \"Union shape: {}\".format(Union.shape)\n",
    "        # add extra ones to union to prevent 0/0\n",
    "        cIoU = Inter / (Union + tf.ones([batch_size], dtype=tf.float32))\n",
    "        class_validNum = tf.greater_equal(tf.reduce_sum(y[:, :, :, c], axis=[1, 2]),\n",
    "                                          tf.ones([batch_size], dtype=np.float32))\n",
    "        class_validNum = tf.reduce_sum(tf.cast(class_validNum, tf.float32))\n",
    "        cIoU = tf.reduce_sum(cIoU) / (tf.constant(0.01, dtype=tf.float32) + class_validNum)\n",
    "        classIoU.append(cIoU)\n",
    "        meanIoU = meanIoU + cIoU\n",
    "    valid_classes = tf.greater_equal(tf.reduce_sum(y, axis=[0, 1, 2]), tf.ones([n_classes], dtype=np.float32))\n",
    "    valid_classes = tf.reduce_sum(tf.cast(valid_classes, tf.float32))\n",
    "    meanIoU = meanIoU / (tf.constant(0.01, dtype=tf.float32) + valid_classes)\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy, op1 = tf.metrics.accuracy(tf.argmax(y, 1), tf.argmax(y_pred, 1), name=\"accuracy\")\n",
    "    \n",
    "    # Precision\n",
    "    precision, op2 = tf.metrics.precision(tf.argmax(y, 1), tf.argmax(y_pred, 1), name=\"precision\")\n",
    "    \n",
    "    # Recall\n",
    "    recall, op3 = tf.metrics.recall(tf.argmax(y, 1), tf.argmax(y_pred, 1), name=\"recall\")\n",
    "    \n",
    "    # F-score\n",
    "    fscore = tf.cast(2 * precision * recall / (precision + recall), tf.float32, name=\"fscore\")\n",
    "    \n",
    "# Where to add summary\n",
    "with tf.name_scope(\"summary\") as scope: \n",
    "\n",
    "    # Add gradients to summary  \n",
    "    for gradient, var in gradients:\n",
    "        tf.summary.histogram(var.name + '/gradient', gradient)\n",
    "        \n",
    "    # Add the variables we train to the summary  \n",
    "    for var in train_vars:\n",
    "        tf.summary.histogram(var.name, var)\n",
    "\n",
    "    # Add the loss to the  summary\n",
    "    loss_summary = tf.summary.scalar('Cross_entropy', loss)\n",
    "    \n",
    "    # Add IoU to the summary\n",
    "    iou_summary = tf.summary.scalar('IoU', iou)\n",
    "    \n",
    "    # Add custom IoU to the summary\n",
    "    for c in range(n_classes):\n",
    "        tf.summary.scalar('class{}_IoU'.format(c), classIoU[c])\n",
    "    tf.summary.scalar('meanIoU', meanIoU)\n",
    "      \n",
    "    # Add the accuracy to the summary\n",
    "    tf.summary.scalar('Accuracy', accuracy)\n",
    "      \n",
    "    # Add the fscore to the summary\n",
    "    tf.summary.scalar('Fscore', fscore)\n",
    "\n",
    "    # Merge all summaries together\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    \n",
    "# Initialize the FileWriter\n",
    "writer = tf.summary.FileWriter(filewriter_path)\n",
    "\n",
    "# Global and local variables initializers\n",
    "global_init = tf.global_variables_initializer()\n",
    "local_init = tf.local_variables_initializer()\n",
    "\n",
    "# Initialize a saver for store model checkpoints\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filewriter_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking graph, operations, variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start Tensorflow session\n",
    "with tf.Session() as sess:\n",
    " \n",
    "    # Initialize all variables\n",
    "    sess.run([global_init, local_init])\n",
    "  \n",
    "    # Add the model graph to TensorBoard\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "writer.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for var in tf.trainable_variables():\n",
    "    print(var.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for var in check_vars:\n",
    "    print(var.name, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 224, 224, 2)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create one-hot labels\n",
    "y_train_onehot = labels_to_onehot(y_train, n_classes)\n",
    "y_train_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "# Get the number of training/validation steps per epoch\n",
    "train_batches_per_epoch = np.floor(len(X_train) / batch_size).astype(np.int16)\n",
    "val_batches_per_epoch = np.floor(len(X_val) / batch_size).astype(np.int16)\n",
    "print(train_batches_per_epoch)\n",
    "print(val_batches_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-08 07:25:27.580141 Start training...\n",
      "2019-11-08 07:25:27.816320 Open Tensorboard at --logdir tf_logs_unet/VGG16_UNet-run-07251208112019/\n",
      "2019-11-08 07:25:27.816357 Epoch number: 1\n",
      "2019-11-08 07:29:27.614551 Saving checkpoint of model...\n",
      "2019-11-08 07:29:27.614736 Epoch number: 2\n",
      "2019-11-08 07:33:31.591807 Saving checkpoint of model...\n",
      "2019-11-08 07:33:31.591956 Epoch number: 3\n",
      "2019-11-08 07:37:21.566085 Saving checkpoint of model...\n",
      "2019-11-08 07:37:21.566218 Saving the final model...\n",
      "2019-11-08 07:37:21.566242 Final model saved at /tmp/vgg16_unet/vgg16_unet_final_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Start Tensorflow session\n",
    "with tf.Session() as sess:\n",
    " \n",
    "    # Initialize all variables\n",
    "    sess.run([global_init, local_init])\n",
    "  \n",
    "    # Add the model graph to TensorBoard\n",
    "    writer.add_graph(sess.graph)\n",
    "  \n",
    "    print(\"{} Start training...\".format(datetime.now()))\n",
    "    print(\"{} Open Tensorboard at --logdir {}\".format(datetime.now(), filewriter_path))\n",
    "  \n",
    "    # Loop over number of epochs\n",
    "    for epoch in range(n_epochs):    \n",
    "        print(\"{} Epoch number: {}\".format(datetime.now(), epoch + 1))\n",
    "        \n",
    "        for batch_index in range(train_batches_per_epoch):\n",
    "            # Get a new batch of images and labels\n",
    "            X_batch, y_batch = batch_with_reps(X_train, y_train_onehot, batch_size)\n",
    "            feed_dict = {X: X_batch, y: y_batch}\n",
    "            \n",
    "            # Run the training op\n",
    "            sess.run(training_op, feed_dict=feed_dict)\n",
    "            \n",
    "            # Check the performance every 10 batches\n",
    "            if batch_index % 10 == 0:\n",
    "                feed_dict={X: X_batch, y: y_batch}\n",
    "                step = epoch * train_batches_per_epoch + batch_index\n",
    "                \n",
    "                # Update ops to correctly calculate tf_metrics\n",
    "                sess.run([op, op1, op2, op3], feed_dict=feed_dict)\n",
    "                \n",
    "                # Calculate tf_metrics and write the summary\n",
    "                summary_str = sess.run(merged_summary, feed_dict=feed_dict)\n",
    "                writer.add_summary(summary_str, step)\n",
    "                \n",
    "                # Flush the event file to disk\n",
    "                writer.flush()\n",
    "        \"\"\"\n",
    "        # Validate the model on the validation set using batches after each epoch\n",
    "        print(\"{} Start validation\".format(datetime.now()))\n",
    "        val_acc = 0.\n",
    "        val_count = 0\n",
    "        for _ in range(val_batches_per_epoch):\n",
    "            X_batch, y_batch = batch_with_reps(X_val_std, y_val_onehot, batch_size)\n",
    "            acc = sess.run(accuracy, feed_dict={X: X_batch, y: y_batch, keep_prob: 1.})\n",
    "            val_acc += acc\n",
    "            val_count += 1\n",
    "        val_acc /= val_count\n",
    "        print(\"{} Validation Accuracy = {:.4f}\".format(datetime.now(), val_acc))\n",
    "        \"\"\"\n",
    "        # Save checkpoint of the model\n",
    "        print(\"{} Saving checkpoint of model...\".format(datetime.now()))  \n",
    "        \n",
    "        #checkpoint_name = os.path.join(checkpoint_path, 'model_epoch'+str(epoch+1)+'.ckpt')\n",
    "        #saver.save(sess, checkpoint_name)  \n",
    "        \n",
    "        #print(\"{} Model checkpoint saved at {}\".format(datetime.now(), checkpoint_name))\n",
    "    \n",
    "    # Save the final model\n",
    "    print(\"{} Saving the final model...\".format(datetime.now()))\n",
    "    \n",
    "    #saver.save(sess, final_model_path)\n",
    "    \n",
    "    print(\"{} Final model saved at {}\".format(datetime.now(), final_model_path))\n",
    "    \n",
    "    # Isolate the variables stored behind the scenes by the metric operation\n",
    "    #stream_vars = [i for i in tf.local_variables() if 'fscore' in i.name] # variables associated with F score\n",
    "    \n",
    "    # Initialize/reset the selected running variables\n",
    "    #stream_vars_init = tf.variables_initializer(var_list=stream_vars)\n",
    "    #sess.run(stream_vars_init)\n",
    "    \n",
    "    # Validate the final model on the entire validation set\n",
    "    proba_val, fscore_val, iou_val = sess.run([y_proba, fscore, iou], feed_dict={X: X_val})\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(np.sum(proba_val[:,:,:,0]==0.7339438))\n",
    "# print(25*224*224)\n",
    "# proba_val[proba_val[:,:,:,0]!=0.7339438]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "proba_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = (proba_val[:,1] >= 0.5)\n",
    "print(\"Total:\", pred.size)\n",
    "print(\"Condition positive\", y_val.sum())\n",
    "print(\"Predicted positive:\", pred.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "print(\"Precision:\", precision_score(y_val, pred))\n",
    "print(\"Recall:\", recall_score(y_val, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_val, pred, labels = [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_val, pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(y_val, proba[:,1])\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Posiitve Rate')\n",
    "    \n",
    "plot_roc_curve(fpr, tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "prec, rec, threshold = precision_recall_curve(y_val, proba[:,1])\n",
    "\n",
    "def plot_roc_curve(prec, rec, label=None):\n",
    "    plt.plot(rec, prec, linewidth=2, label=label)\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    \n",
    "plot_roc_curve(prec, rec)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test images from ImageNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from caffe_classes import class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_images = prep.load_imagenet_images(test_on_imagenet, input_height, input_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the pretrained model on ImageNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Add the model graph to TensorBoard\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    # Create figure handle\n",
    "    fig2 = plt.figure(figsize=(15,6))\n",
    "    \n",
    "    # Loop over all images\n",
    "    for i, image in enumerate(test_images):\n",
    "        \n",
    "        img = image.reshape((1, input_height, input_width, 3))\n",
    "        \n",
    "        # Run the session and calculate the class probability\n",
    "        proba = sess.run(y_proba, feed_dict={X: img})\n",
    "        \n",
    "        # Get the class name of the class with the highest probability\n",
    "        class_name = class_names[np.argmax(proba)]\n",
    "        \n",
    "        # Plot image with class name and prob in the title\n",
    "        #fig2.add_subplot(1,3,i+1)\n",
    "        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(\"Class: \" + class_name + \", probability: %.4f\" %proba[0,np.argmax(proba)])\n",
    "        plt.axis('off')\n",
    "        \n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
